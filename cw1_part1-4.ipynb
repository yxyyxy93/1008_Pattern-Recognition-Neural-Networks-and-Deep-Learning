{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# import the rao data\n",
    "iris = datasets.load_iris()\n",
    "# field iris.data is an array, each column of which is a featureï¼Œ vector for one sample, \n",
    "# field iris.target with is a column vector defining the class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.43478261 7.91304348 4.86956522 7.91304348 4.43478261 5.73913043\n",
      "  7.91304348]\n",
      " [4.25       2.5        4.25       2.25       3.         4.25\n",
      "  2.25      ]\n",
      " [2.33333333 7.         1.66666667 3.66666667 7.         1.66666667\n",
      "  7.        ]\n",
      " [2.25       0.25       1.         2.25       0.25       2.25\n",
      "  0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Define s1, s2, s3, s4, s5, s6, s7 to be the 1st to 7th digits of your KCL student ID number,\n",
    "s1, s2, s3, s4, s5, s6, s7 = 1, 9, 2, 9, 1, 4, 9\n",
    "# create this test set, Xtest\n",
    "\n",
    "S_test=np.array([[s1, s2, s3, s4, s5, s6, s7],[s2, s3, s4, s5, s6, s7, s1],\n",
    "[s3, s4, s5, s6, s7, s1, s2],[s4, s5, s6, s7, s1, s2, s3]]);\n",
    "\n",
    "S_test=S_test/np.array([2.3,4,1.5,4]).reshape(-1,1);\n",
    "X_test=S_test+np.array([4,2,1,0]).reshape(-1,1)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(4, 7)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)\n",
    "print(X_test.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3:  [0 2 0 1 2 0 2]\n",
      "k=7:  [0 2 0 1 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# knn k=3 or 7\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "X_train, y_train = iris.data, iris.target\n",
    "\n",
    "# k=3\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test.transpose())\n",
    "print('k=3: ', y_pred)\n",
    "\n",
    "# k=7\n",
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test.transpose())\n",
    "print('k=7: ', y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k = 3, the predicted sample in Xtest is [0 2 0 1 2 0 2]. For k = 7, the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  2],\n",
       "       [ 1,  1,  2],\n",
       "       [ 1,  2,  1],\n",
       "       [-1,  3, -1],\n",
       "       [-1,  2,  1],\n",
       "       [-1,  3,  2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt = np.array([[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]])\n",
    "y_class = np.array([[1], [1], [1], [-1], [-1], [-1]])\n",
    "# Using Augmented notation and sample normalisation, dataset is:\n",
    "yt = np.append(y_class, Xt*y_class, axis=1)\n",
    "\n",
    "# yt = np.array([[1, 0, 2], [1, 1, 2], [1, 2, 1], [-1, 3, -1], [-1, 2, 1], [-1, 3, 2]])\n",
    "\n",
    "yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [9],\n",
       "       [2],\n",
       "       [9],\n",
       "       [1],\n",
       "       [4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial values\n",
    "at = np.array([1, 0, 0])\n",
    "# learing rate\n",
    "lr = 0.1\n",
    "\n",
    "#b = np.array([[1], [1], [1], [1], [1], [1]])\n",
    "# margin vector \n",
    "b = np.array([[s1], [s2], [s3], [s4], [s5], [s6]])\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Sequential Widrow-Hoff Learning Algorithm, weights are updated\n",
    "def weight_update(a, lr, bk, yk):\n",
    "    a_updated = a + lr * (bk - np.dot(a.transpose(), yk)) * yk\n",
    "    return a_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[ 0.8  0.6 -0.2]\n",
      "[ 0.72  0.76 -0.12]\n",
      "[ 0.752  0.664 -0.184]\n",
      "[ 0.8136  0.664  -0.0608]\n",
      "[ 0.778   0.6284 -0.132 ]\n",
      "[ 0.68772  0.44784 -0.22228]\n",
      "[ 0.675528  0.484416 -0.234472]\n",
      "[ 0.5814112  0.6726496 -0.1403552]\n",
      "[ 0.59699392  0.62590144 -0.17152064]\n"
     ]
    }
   ],
   "source": [
    "# margin vector not changed\n",
    "b = np.array([[1], [1], [1], [1], [1], [1]])\n",
    "for k in range(0, 12):\n",
    "    at = weight_update(at, lr, b[k%6], yt[k%6])\n",
    "    print(at)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.67159866  0.62590144 -0.02231117]\n",
      "[1.44631088 1.40061366 1.52711328]\n",
      "[1.06884573 0.64568337 1.14964813]\n",
      "[0.14070135 3.4301165  0.22150375]\n",
      "[ 0.73480489  2.24190942 -0.37259978]\n",
      "[ 0.85937727  1.86819228 -0.62174454]\n",
      "[ 0.99778845  1.86819228 -0.34492218]\n",
      "[1.68017482 2.55057864 1.01985055]\n",
      "[1.10005655 1.39034211 0.43973228]\n",
      "[ 0.4631803   3.30097086 -0.19714397]\n",
      "[ 0.95734205  2.31264737 -0.69130571]\n",
      "[ 1.01714091  2.13325078 -0.81090344]\n"
     ]
    }
   ],
   "source": [
    "# margin vector changed\n",
    "b = np.array([[s1], [s2], [s3], [s4], [s5], [s6]])\n",
    "\n",
    "for k in range(0, 12):\n",
    "    at = weight_update(at, lr, b[k%6], yt[k%6])\n",
    "    print(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the parameters of the discriminant function learnt at each iteration are shown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly separable data set\n",
    "Xt = np.array([[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]])\n",
    "y_class = np.array([[1], [1], [1], [0], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values and a learning rate\n",
    "theta = -1\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  2.],\n",
       "       [ 1.,  1.,  2.],\n",
       "       [ 1.,  2.,  1.],\n",
       "       [ 1., -3.,  1.],\n",
       "       [ 1., -2., -1.],\n",
       "       [ 1., -3., -2.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented notation\n",
    "w = [-theta, w1, w2]\n",
    "xt = np.append(np.ones((6, 1)), Xt, axis=1)\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heaviside function\n",
    "def H(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    elif x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Sequential Delta Learning Algorithm, weights-updating function\n",
    "def weight_update(w, lr, t, y, xt):\n",
    "    w_updated = w + lr * (t-y)*xt\n",
    "    return w_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[ 0.  3. -1.]\n",
      "[ 0.  3. -1.]\n",
      "[ 0.  3. -1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n",
      "[1. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, 13):\n",
    "    y = H(np.dot(w, xt[k%6].transpose()))\n",
    "    w = weight_update(w, lr, y_class[k%6], y, xt[k%6])\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change the intial values \n",
    "theta = -s3\n",
    "w1 = -s4\n",
    "w2 = s5\n",
    "\n",
    "w = [-theta, w1, w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. -9.  1.]\n",
      "[ 3. -8.  3.]\n",
      "[ 4. -6.  4.]\n",
      "[ 3. -3.  3.]\n",
      "[ 2. -1.  4.]\n",
      "[ 2. -1.  4.]\n",
      "[ 2. -1.  4.]\n",
      "[ 2. -1.  4.]\n",
      "[ 2. -1.  4.]\n",
      "[1. 2. 3.]\n",
      "[1. 2. 3.]\n",
      "[1. 2. 3.]\n",
      "[1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, 13):\n",
    "    y = H(np.dot(w, xt[k%6].transpose()))\n",
    "    w = weight_update(w, lr, y_class[k%6], y, xt[k%6])\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning has converged whithin 13 iterations, so required parameters are w = (1, 2, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Import from sklearn.metrics the function confusion matrix \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# sing five epochs fits the model to the data\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# normalize the image data in x_train and x_test (from 0 to 255)\n",
    "# rescale this from 0 to 1\n",
    "\n",
    "x_train = x_train/255\n",
    "\n",
    "x_test= x_test/255\n",
    "# split the training data into validation and training data \n",
    "# for cross validation\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train,y_train, test_size = 0.2, random_state = 7788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the image to have a look \n",
    "print(x_train.shape)\n",
    "\n",
    "image = x_train[10,:]\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(48000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "nrows = 28\n",
    "ncols = 28\n",
    "batch = 512\n",
    "\n",
    "image_shape = (nrows, ncols, 1) # 3d with rows and columns\n",
    "# formate the x_train, x_test and x_validate sets.\n",
    "print (x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0],*image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0],*image_shape)\n",
    "\n",
    "print (x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.4606 - acc: 0.8743 - val_loss: 0.2358 - val_acc: 0.9332\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1977 - acc: 0.9445 - val_loss: 0.1704 - val_acc: 0.9507\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1439 - acc: 0.9605 - val_loss: 0.1465 - val_acc: 0.9572\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1115 - acc: 0.9684 - val_loss: 0.1201 - val_acc: 0.9650\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.0875 - acc: 0.9761 - val_loss: 0.1071 - val_acc: 0.9677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a0802ef438>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Keras Sequential approach to building neural net.\n",
    "# Use the ReLU activation function for the layer\n",
    "# Use 512 (2^9) units for the hidden layer.\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Flatten(), # flatten out the layers\n",
    "    Dense(512,activation='relu'),\n",
    "    Dense(10,activation = 'softmax')\n",
    "])\n",
    "\n",
    "# complie the mode \n",
    "model_cnn.compile(loss ='sparse_categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=0.001), \n",
    "                  metrics =['accuracy'])\n",
    "\n",
    "model_cnn.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate, y_validate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 972    0    2    1    2    0    0    2    1    0]\n",
      " [   0 1120    3    1    0    1    3    1    6    0]\n",
      " [   4    1 1002    1    2    0    2    6   14    0]\n",
      " [   0    0    5  984    0    3    1    6   10    1]\n",
      " [   2    0    5    1  953    0    3    4    2   12]\n",
      " [   5    1    0   10    2  856    7    1    9    1]\n",
      " [   6    3    3    1    9    5  923    1    7    0]\n",
      " [   2    6   13    3    1    1    0  993    4    5]\n",
      " [   4    0    3    7    3    3    2    3  948    1]\n",
      " [   5    7    1    8   13    1    1    9   11  953]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.97      0.99      0.98       980\n",
      "     Trouser       0.98      0.99      0.99      1135\n",
      "    Pullover       0.97      0.97      0.97      1032\n",
      "       Dress       0.97      0.97      0.97      1010\n",
      "        Coat       0.97      0.97      0.97       982\n",
      "      Sandal       0.98      0.96      0.97       892\n",
      "       Shirt       0.98      0.96      0.97       958\n",
      "     Sneaker       0.97      0.97      0.97      1028\n",
      "         Bag       0.94      0.97      0.95       974\n",
      "  Ankle boot       0.98      0.94      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix of the errors on the test data.\n",
    "y_pred = model_cnn.predict(x_test)\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred.argmax(axis=1))\n",
    "\n",
    "print(matrix)\n",
    "\n",
    "names= [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    " \n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred.argmax(axis=1), target_names=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is called MNIST. ï¼´here are ten classes, showing pictures of headwriting digitals. These items are stored in low-resolution 28X28. images made up of integers between zero and 255, indicating the grey scale of the pixel they are representing. \n",
    "\n",
    "CNN model was chosen for implementation. We wrote a script implementing a one hidden-layer neural network to learn various digitals items. Used the Keras Sequential approach to build neural net. Used the ReLU activation function for the layer. 512 units for the hidden layer were applied. Used 5 epochs to fit the model to the data.\n",
    "\n",
    "The accuracy on my test set is 97%. Also the results on test data are showed by confusion matrix. these names are in order ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
